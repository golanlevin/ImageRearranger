{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ImageRearranger* Jupyter Notebook\n",
    "\n",
    "*This notebook arranges a grid of images into a mosaic according to their similarity.*\n",
    "\n",
    "![overview.png](overview.png)\n",
    "\n",
    "[This notebook](https://github.com/golanlevin/ImageRearranger/blob/master/ImageRearranger.ipynb) (ImageRearranger.ipynb):\n",
    "\n",
    "1. Loads a collection of images (either as a zipped file, a directory, or in the form of a single mosaic image).\n",
    "2. Computes high-dimensional features which describe the images (either using an [image pyramid](https://en.wikipedia.org/wiki/Pyramid_(image_processing)), or a neural network).\n",
    "3. [Reduces the dimensionality](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) of those features to a 2D point cloud (either using [UMAP](https://umap-learn.readthedocs.io/en/latest/) or [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding))\n",
    "4. Rectifies the 2D point cloud into a grid (by solving the [Linear Assignment Problem](https://en.wikipedia.org/wiki/Assignment_problem)), using that grid to produce an ordered mosaic of the input images.\n",
    "5. Saves the final mosaic image, as well as JSON files containing the 2D point cloud and grid locations.\n",
    "\n",
    "A very similar [Google Colab version of this notebook is available here](https://colab.research.google.com/drive/1rgxYnSziGuToW0oLbmzSwq470e6nVXHC?usp=sharing).\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* You will need Python 3.11 or newer. *This notebook was developed with Python 3.13 on MacOSX 14.5.*\n",
    "  * First, **check** if Python3 is installed. Run: `python3 --version`\n",
    "  * If you see output like `Python 3.x.x`, then Python is installed. If not, install it via Homebrew: `brew install python`\n",
    "  * Then **verify**: `python3 --version`\n",
    "* **Download** [this repository](https://github.com/golanlevin/ImageRearranger):\n",
    "  * (**Option 1: Download as ZIP**) In GitHub, **click** on the green \"Code\" button in the upper right and **select** \"Download ZIP\" from the pulldown that appears). This will download a zip file to your computer. **Unzip** the compressed file. I recommend that you **rename** the folder to `ImageRearranger/`.\n",
    "  * (**Option 2: Clone via Git**) In your terminal, **navigate** to where you want to save the repository, and run this command to clone it: `git clone https://github.com/golanlevin/ImageRearranger.git`\n",
    "* At your Terminal prompt, **change directory** to that folder: `cd ImageRearranger`\n",
    "* Because this notebook requires the installation of numerous Python libraries, *working in a virtual environment (venv) is extremely strongly recommended*. **Create** a virtual environment, e.g. `mosaicVenv`:\n",
    "  * Mac: `python3 -m venv mosaicVenv`\n",
    "  * Win: `python -m venv mosaicVenv`\n",
    "* **Activate** the `mosaicVenv` virtual environment:\n",
    "  * Mac: `source mosaicVenv/bin/activate`\n",
    "  * Win: `mosaicVenv\\Scripts\\activate`\n",
    "* Before we install necessary libraries, it's optional but recommended that you **upgrade** pip first: `pip install --upgrade pip`\n",
    "* **Install** all required packages (making sure that `mosaicVenv` is activated first!): `pip install -r requirements.txt`.\n",
    "  * This will install: `numpy scipy scikit-learn Pillow jupyter matplotlib opencv-python scikit-image umap-learn torch torchvision\n",
    "git+https://github.com/gatagat/lap.git`\n",
    "  * You won't need to re-install libraries the next time you activate the virtual environment.\n",
    "* You should now be able to **launch** the [ImageRearranger.ipynb](ImageRearranger.ipynb) Jupyter Notebook (if you haven't already) with `jupyter notebook`\n",
    "  * This will open `http://localhost:8888/tree`\n",
    "  * From there, **open** `http://localhost:8888/notebooks/ImageRearranger.ipynb`\n",
    "* **Step through** this notebook! You can step through this notebook using options in the Run menu. I recommend stepping through cell by cell with **Shift-Return**.\n",
    "\n",
    "---\n",
    "\n",
    "### Quickstart\n",
    "\n",
    "This assumes you have already constructed the virtual environment, and installed the libraries, etc.:\n",
    "\n",
    "* `cd ImageRearranger` \n",
    "* `source mosaicVenv/bin/activate`\n",
    "* `jupyter notebook`\n",
    "\n",
    "---\n",
    "\n",
    "### Credits\n",
    "\n",
    "* Based on Kyle McDonald's [ImageRearranger](https://github.com/kylemcdonald/ImageRearranger/tree/master?tab=readme-ov-file).\n",
    "* Includes code from Kyle McDonald's [python-utils](https://github.com/kylemcdonald/python-utils) repository. \n",
    "* Inspired by [this collection](https://twitter.com/JUSTIN_CYR/status/829196024631681024) of pixel art by Justin Cyr.\n",
    "* Demo includes tiles of the [Kress Collection](https://www.kressfoundation.org/kress-collection) at the US National Gallery of Art.\n",
    "* Updates & Extensions by Golan Levin, February 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Settings, Imports, and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageRearranger\n",
    "# Arranges a grid of images into a mosaic according to their similarity.\n",
    "\n",
    "# Practical limits on this tool: \n",
    "# - Optimizing a grid for more than ~5K points will start to become EXTREMELY slow. \n",
    "# - Because the cost matrix is n^2 in memory, ~45K+ points exceed a 32GB RAM ceiling.\n",
    "\n",
    "# -----------------------------------------------\n",
    "# GLOBAL SETTINGS\n",
    "# This program loads a pre-made mosaic image or a folder of square images.\n",
    "\n",
    "LOAD_FROM_FOLDER = True  # ‚úÖ Set to True to load tiles from a folder instead of a mosaic.\n",
    "LOAD_FOLDER_FROM_ZIP = False # ‚úÖ Set to True to extract folder from a zip file. \n",
    "\n",
    "IMAGE_FILENAME = \"inputs/src_cyr_32.png\"  # ‚úÖ Used if LOAD_FROM_FOLDER = False\n",
    "IMAGE_FOLDER = \"inputs/nga_kress_32\"      # ‚úÖ Used if LOAD_FROM_FOLDER = True\n",
    "IMAGE_FOLDER_ZIPFILE = \"inputs/nga_kress_32.zip\" # ‚úÖ Used if LOAD_FROM_FOLDER = True\n",
    "TILE_SIZE = 32                            # ‚úÖ Define tile size for image processing\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Feature extraction method\n",
    "# Choose between \"pyramid\" (multi-scale image blurring) or \"cnn\" (deep learning features)\n",
    "FEATURE_METHOD = \"cnn\"  # ‚úÖ Options: \"pyramid\" or \"cnn\"\n",
    "\n",
    "# Dimensionality reduction method\n",
    "REDUCTION_METHOD = \"TSNE\" # ‚úÖ Options: \"UMAP\" or \"TSNE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# NOTEBOOK SETUP\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from umap import UMAP\n",
    "\n",
    "# Improve matplotlib rendering quality for retina displays\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "# Improve image rendering in Jupyter\n",
    "IPython.core.display.display_html(\n",
    "    IPython.core.display.HTML(\"<style>img{image-rendering: pixelated}</style>\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# IMAGE UTILITY FUNCTIONS\n",
    "\n",
    "# Reads an image from a file using OpenCV.\n",
    "# Returns the loaded image as a NumPy array.\n",
    "def imread(filename, mode=None):\n",
    "    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image file '{filename}' not found.\")\n",
    "    if mode == 'rgb':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    elif mode == 'gray':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Displays an image (a NumPy array) in a Jupyter notebook using Matplotlib.\n",
    "# Increases display resolution if `retina` is True.\n",
    "def imshow(img, retina=False):\n",
    "    if img is None:\n",
    "        raise ValueError(\"Input image is None.\")\n",
    "    plt.figure(figsize=(6, 6) if retina else (4, 4))\n",
    "    cmap = 'gray' if len(img.shape) == 2 else None\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Loads all images from a folder; uses the `imread()` function above.\n",
    "def load_images_from_folder(folder_path, target_size=32, debug=True):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        target_size (int): Target size (assumes square images, default=32).\n",
    "        debug (bool): If True, prints debug information.\n",
    "    Returns:\n",
    "        np.ndarray: Array of images in (num_images, 32, 32, 3) format.\n",
    "        list: List of successfully loaded image filenames (sorted order).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    filenames = []\n",
    "    \n",
    "    # Get all valid image files (JPG and PNG), sorted numerically\n",
    "    valid_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "    files = sorted(\n",
    "        [f for f in os.listdir(folder_path) \n",
    "         if os.path.splitext(f.lower())[1] in valid_extensions]\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Found {len(files)} potential images in '{folder_path}':\")\n",
    "        print(files[:10])  # Print first 10 filenames (to check sorting)\n",
    "\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Use the existing `imread()` function\n",
    "            img = imread(file_path, mode='rgb')\n",
    "\n",
    "            # Ensure image is the target size (resize if necessary)\n",
    "            if img.shape[:2] != (target_size, target_size):\n",
    "                img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_LANCZOS4)\n",
    "            \n",
    "            # Store image and filename\n",
    "            images.append(img)\n",
    "            filenames.append(filename)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            if debug:\n",
    "                print(f\"‚ùå Skipping missing file: {filename}\")\n",
    "            continue  # Skip missing/corrupt files\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"‚ö†Ô∏è Error loading {filename}: {e}\")\n",
    "            continue  # Skip problematic files\n",
    "    if debug:\n",
    "        print(f\"‚úÖ Successfully loaded {len(images)} images.\")\n",
    "    return np.array(images), filenames\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# IMAGE MOSAIC FUNCTIONS\n",
    "\n",
    "# Swap axes of an array (supports NumPy and PyTorch).\n",
    "def swapaxes(x, a, b):\n",
    "    try:\n",
    "        return x.swapaxes(a, b)\n",
    "    except AttributeError:  # Support PyTorch tensors\n",
    "        return x.transpose(a, b)\n",
    "\n",
    "\n",
    "# Arranges a batch of images into a mosaic grid.\n",
    "def make_mosaic(x, nx=None, ny=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x (np.ndarray): Image batch of shape (n, h, w) or (n, h, w, c).\n",
    "        nx (int): Number of columns in the mosaic.\n",
    "        ny (int): Number of rows in the mosaic.\n",
    "    Returns:\n",
    "        np.ndarray: Mosaic image.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        x = np.asarray(x)\n",
    "\n",
    "    n, h, w = x.shape[:3]\n",
    "    has_channels = len(x.shape) > 3\n",
    "    c = x.shape[3] if has_channels else None\n",
    "\n",
    "    if nx is None and ny is None:\n",
    "        ny = int(np.sqrt(n))  # Default to a roughly square layout\n",
    "        nx = (n + ny - 1) // ny  # Ensure enough columns\n",
    "    elif ny is None:\n",
    "        ny = n // nx\n",
    "    elif nx is None:\n",
    "        nx = n // ny\n",
    "\n",
    "    end_shape = (w, c) if has_channels else (w,)\n",
    "    mosaic = x.reshape(ny, nx, h, *end_shape)\n",
    "    mosaic = swapaxes(mosaic, 1, 2)\n",
    "    mosaic = mosaic.reshape(ny * h, nx * w, c) if has_channels else mosaic.reshape(ny * h, nx * w)\n",
    "\n",
    "    return mosaic\n",
    "\n",
    "\n",
    "# Splits a mosaic image into individual images.\n",
    "# Note: `//` is the Python floored-division operator. \n",
    "def unmake_mosaic(mosaic, nx=None, ny=None, w=None, h=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        mosaic (np.ndarray): The mosaic image.\n",
    "        nx (int): Number of images per row.\n",
    "        ny (int): Number of images per column.\n",
    "        w (int): Width of each sub-image.\n",
    "        h (int): Height of each sub-image.\n",
    "    Returns:\n",
    "        np.ndarray: Array of split images.\n",
    "    \"\"\"\n",
    "    hh, ww = mosaic.shape[:2]\n",
    "\n",
    "    if nx is not None or ny is not None:\n",
    "        if nx is None:\n",
    "            h = hh // ny\n",
    "            w = h\n",
    "            nx = ww // w\n",
    "        elif ny is None:\n",
    "            w = ww // nx\n",
    "            h = w\n",
    "            ny = hh // h\n",
    "        else:\n",
    "            w = ww // nx\n",
    "            h = hh // ny\n",
    "    elif w is not None or h is not None:\n",
    "        if w is None:\n",
    "            w = h\n",
    "        elif h is None:\n",
    "            h = w\n",
    "        nx = ww // w\n",
    "        ny = hh // h\n",
    "\n",
    "    end_shape = (w, mosaic.shape[2]) if len(mosaic.shape) > 2 else (w,)\n",
    "\n",
    "    x = mosaic.reshape(ny, h, nx, *end_shape)\n",
    "    x = swapaxes(x, 1, 2)\n",
    "    x = x.reshape(-1, h, *end_shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# IMAGE PLOTTING FUNCTIONS\n",
    "\n",
    "# Places images at given (x, y) coordinates onto a blank canvas.\n",
    "def plot_images(images, xy, blend=np.maximum, canvas_shape=(512, 512), fill=0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        images (np.ndarray): Array of images to place.\n",
    "        xy (np.ndarray): (x, y) positions for each image.\n",
    "        blend (func): Blending function for overlapping images (default: np.maximum).\n",
    "        canvas_shape (tuple): Shape of output canvas (height, width, channels).\n",
    "        fill (int): Fill value for empty pixels (default: 0).\n",
    "    Returns:\n",
    "        np.ndarray: Final composed image.\n",
    "    \"\"\"\n",
    "    h, w = images.shape[1:3]\n",
    "    if images.ndim == 4:\n",
    "        canvas_shape = (canvas_shape[0], canvas_shape[1], images.shape[3])\n",
    "\n",
    "    min_xy = np.amin(xy, 0)\n",
    "    max_xy = np.amax(xy, 0)\n",
    "\n",
    "    min_canvas = np.array((0, 0))\n",
    "    max_canvas = np.array((canvas_shape[0] - h, canvas_shape[1] - w))\n",
    "\n",
    "    xy_mapped = min_canvas + (xy - min_xy) * (max_canvas - min_canvas) / (max_xy - min_xy)\n",
    "    xy_mapped = xy_mapped.astype(int)\n",
    "\n",
    "    canvas = np.full(canvas_shape, fill)\n",
    "    for image, pos in zip(images, xy_mapped):\n",
    "        x_off, y_off = pos\n",
    "        sub_canvas = canvas[y_off:y_off+h, x_off:x_off+w]\n",
    "        sub_image = image[:h, :w]\n",
    "        try:\n",
    "            canvas[y_off:y_off+h, x_off:x_off+w] = blend(sub_canvas, sub_image)\n",
    "        except ValueError:\n",
    "            print(pos, h, w, min_canvas, max_canvas)\n",
    "            raise\n",
    "\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load a collection of images \n",
    "### Images can be loaded as a zipped collection, a directory, or as a single mosaic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# LOAD IMAGE DATA (MOSAIC OR FOLDER)\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "if LOAD_FROM_FOLDER:\n",
    "\n",
    "    if (LOAD_FOLDER_FROM_ZIP):\n",
    "        # Check if the folder exists\n",
    "        if not os.path.exists(IMAGE_FOLDER):\n",
    "            print(f\"üì¶ Extracting {IMAGE_FOLDER_ZIPFILE}...\")\n",
    "            with zipfile.ZipFile(IMAGE_FOLDER_ZIPFILE, 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"inputs\")  # ‚úÖ Extract into the same folder\n",
    "            print(f\"‚úÖ Extracted images to {IMAGE_FOLDER}\")\n",
    "            # Delete the unwanted __MACOSX folder if it exists\n",
    "            UNWANTED_MACOSX_PATH = \"inputs/__MACOSX\"\n",
    "            if os.path.exists(UNWANTED_MACOSX_PATH):\n",
    "                shutil.rmtree(UNWANTED_MACOSX_PATH)\n",
    "    \n",
    "    # Load images from the specified folder\n",
    "    print(f\"Loading images from folder: {IMAGE_FOLDER}\")\n",
    "    images, filenames = load_images_from_folder(IMAGE_FOLDER, TILE_SIZE, False)\n",
    "    print(f\"Loaded {len(images)} tiles from {IMAGE_FOLDER}\")\n",
    "\n",
    "    # Construct a mosaic from the loaded tiles for visualization\n",
    "    aspectRatio = 1.618 # Golden Ratio\n",
    "    nx = round(np.sqrt(aspectRatio * len(images)))  # Estimate layout\n",
    "    ny = (len(images) + nx - 1) // nx  # Ensure enough rows\n",
    "\n",
    "    # Compute the number of missing images for a complete grid\n",
    "    required_images = nx * ny\n",
    "    n_loaded_images = len(images)\n",
    "    missing_tiles = required_images - n_loaded_images\n",
    "    \n",
    "    if missing_tiles > 0:\n",
    "        print(f\"‚ö†Ô∏è Missing {missing_tiles} tiles for complete grid; padding with black tiles.\")\n",
    "        black_tiles = np.zeros((missing_tiles, TILE_SIZE, TILE_SIZE, 3), dtype=np.uint8)  \n",
    "        images = np.concatenate((images, black_tiles), axis=0)  # Concatenate missing tiles\n",
    "\n",
    "    # Construct and display a \"mosaic\" image from the loaded tiles.\n",
    "    img = make_mosaic(images, nx=nx, ny=ny)  \n",
    "    print(f\"Constructed mosaic from {len(images)} tiles.\")\n",
    "    imshow(img, retina=True)\n",
    "    \n",
    "else:\n",
    "    # Load full mosaic image\n",
    "    print(f\"Loading mosaic image: {IMAGE_FILENAME}\")\n",
    "    img = imread(IMAGE_FILENAME, 'rgb')  \n",
    "    imshow(img, retina=True)\n",
    "\n",
    "    # Convert mosaic image into individual tiles\n",
    "    # - `img` is the full input image (assumed to be a mosaic of tiles).\n",
    "    # - `unmake_mosaic(img, w=TILE_SIZE)` extracts individual square tiles.\n",
    "    images = unmake_mosaic(img, w=TILE_SIZE)\n",
    "    n_loaded_images = len(images)  # Should be `nx * ny`\n",
    "    filenames = [f\"tile_{i:04d}\" for i in range(len(images))]\n",
    "\n",
    "\n",
    "# Compute number of columns (nx) and rows (ny) in the mosaic\n",
    "nx = img.shape[1] // TILE_SIZE  # Number of tiles per row\n",
    "ny = img.shape[0] // TILE_SIZE  # Number of tiles per column\n",
    "\n",
    "# Print the total number of extracted images\n",
    "print(f\"‚úÖ Processing {len(images)} images, each of size {TILE_SIZE}√ó{TILE_SIZE} pixels.\")\n",
    "print(f\"üü¢ Mosaic dimensions: {nx}√ó{ny} tiles ({img.shape[1]}√ó{img.shape[0]} pixels).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compute high-dimensional image descriptions\n",
    "### Methods include an image pyramid or a neural network\n",
    "\n",
    "In this section, each image is represented as a numerical feature vector that captures its visual characteristics. Two methods are available:\n",
    "\n",
    "* Image Pyramid: Uses multi-scale Gaussian blurring to create a structured pixel-based representation.\n",
    "* Neural Network (CNN): Uses a pretrained deep learning model (InceptionV3) to extract perceptual features.\n",
    "\n",
    "These representations serve as input for dimensionality reduction techniques like UMAP or T-SNE in the next Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# FUNCTION TO MAKE BLURRED IMAGE SETS\n",
    "\n",
    "# skimage.filters is imported here because \n",
    "# it's only used in this one place. \n",
    "from skimage.filters import gaussian\n",
    "\n",
    "# Applies Gaussian blur to a batch of images.\n",
    "def build_blurred(images, sigma):\n",
    "    # Parameters:\n",
    "    #    images (np.ndarray): Batch of images.\n",
    "    #    sigma (float): Standard deviation for Gaussian kernel.\n",
    "    # Returns:\n",
    "    #    np.ndarray: Blurred images in uint8 format.\n",
    "    blurred = []\n",
    "    for image in images:\n",
    "        # Apply Gaussian blur (output is normalized to [0, 1])\n",
    "        blurred_image = gaussian(image, sigma=sigma, channel_axis=-1)\n",
    "        \n",
    "        # Scale back to [0, 255] and convert to uint8\n",
    "        blurred_image = np.clip(blurred_image * 255, 0, 255).astype(np.uint8)\n",
    "        blurred.append(blurred_image)\n",
    "\n",
    "    return np.array(blurred)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# GENERATE BLURRED IMAGE SETS\n",
    "# We apply different levels of blur to each image in the dataset.\n",
    "# The goal is to create a \"multi-scale\" representation for UMAP.\n",
    "# Higher `sigma` values result in more aggressive blurring.\n",
    "\n",
    "nx = int(img.shape[1] / TILE_SIZE)  # Number of columns\n",
    "ny = int(img.shape[0] / TILE_SIZE)  # Number of rows\n",
    "blur_levels = [4, 2, 1]    # Different blur intensities\n",
    "\n",
    "# Apply Gaussian blur at different levels and visualize the results\n",
    "blurred_0 = build_blurred(images, blur_levels[0])\n",
    "imshow(make_mosaic(blurred_0, nx=nx, ny=ny), retina=True)  # Most blurred\n",
    "\n",
    "blurred_1 = build_blurred(images, blur_levels[1])\n",
    "imshow(make_mosaic(blurred_1, nx=nx, ny=ny), retina=True)  # Medium blur\n",
    "\n",
    "blurred_2 = build_blurred(images, blur_levels[2])\n",
    "imshow(make_mosaic(blurred_2, nx=nx, ny=ny), retina=True)  # Light blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# CONSTRUCT THE IMAGE FEATURE PYRAMID\n",
    "\n",
    "# Number of individual image patches\n",
    "n = len(images)\n",
    "\n",
    "# ‚ö° Feature Stacking:\n",
    "# We create a feature matrix (`pyr`) where each row represents an image,\n",
    "# and each column contains pixel values from different levels of blurring.\n",
    "# This allows UMAP (in the next step) to learn a representation \n",
    "# that takes multiple scales of detail into account.\n",
    "\n",
    "pyr = np.hstack((\n",
    "    blurred_0.reshape(n, -1),  # Strongly blurred images\n",
    "    blurred_1.reshape(n, -1),  # Moderately blurred images\n",
    "    blurred_2.reshape(n, -1),  # Lightly blurred images\n",
    "    images.reshape(n, -1)      # Original images\n",
    "))\n",
    "\n",
    "# ‚úÖ After this step:\n",
    "# `pyr` is now a 2D array of shape (n, feature_dim), where:\n",
    "#   - `n` is the number of image patches.\n",
    "#   - `feature_dim` is the combined size of all image versions.\n",
    "# This feature representation may be used as input to UMAP/T-SNE in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# FEATURE EXTRACTION!\n",
    "#\n",
    "# Depending on the value of the FEATURE_METHOD setting (see first cell), \n",
    "# we choose the input data for dimensionality reduction. Options: \n",
    "# FEATURE_METHOD == \"cnn\" - Uses a neural network to describe each image with 2048 numbers.\n",
    "# FEATURE_METHOD == \"pyramid\" - Uses an image pyramid instead; the pixels are the features. \n",
    "#\n",
    "# NOTE: The neural net (cnn) option downloads a 104MB model, and can take some time.\n",
    "# Uses InceptionV3 with Reduced Size (107x107 instead of 299x299) to avoid out-of-memory errors.\n",
    "# See https://stackoverflow.com/questions/57421842/image-size-of-256x256-not-299x299-fed-into-inception-v3-model-pytorch-and-wo\n",
    "\n",
    "if FEATURE_METHOD == \"cnn\":\n",
    "    import torch\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    # Select device (use GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load InceptionV3 model\n",
    "    print(\"‚è≥ Loading InceptionV3 model with reduced input size (107x107)...\")\n",
    "    model = models.inception_v3(weights=\"IMAGENET1K_V1\").to(device)\n",
    "    model.fc = torch.nn.Identity()  # Remove final classification layer\n",
    "    model.AuxLogits = None  # Disable auxiliary classifier\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    print(\"‚úÖ InceptionV3 model loaded.\")\n",
    "\n",
    "    # Preprocessing: Resize to 107x107 (instead of 299x299)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((107, 107)),  # Reduce input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "    ])\n",
    "\n",
    "    @torch.no_grad()  # Disable gradient tracking for efficiency\n",
    "    def extract_features(images, batch_size=16):\n",
    "        \"\"\"\n",
    "        Extract CNN features in batches to prevent out-of-memory errors.\n",
    "        Uses a reduced input size (107x107) for efficiency.\n",
    "        Parameters:\n",
    "            images (np.ndarray): Image batch of shape (num_images, 32, 32, 3).\n",
    "            batch_size (int): Number of images per batch.\n",
    "        Returns:\n",
    "            np.ndarray: Extracted feature vectors, shape (num_images, 2048).\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "\n",
    "        # Process images in mini-batches\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch_images = images[i : i + batch_size]  # Slice a batch\n",
    "            \n",
    "            # Convert images to tensors\n",
    "            images_resized = []\n",
    "            for img in batch_images:\n",
    "                img_pil = transforms.ToPILImage()(img)  # Convert NumPy to PIL Image\n",
    "                img_tensor = preprocess(img_pil).unsqueeze(0).to(device)  # Preprocess + add batch dim\n",
    "                images_resized.append(img_tensor)\n",
    "\n",
    "            batch_tensor = torch.cat(images_resized, dim=0)  # Stack into a batch tensor\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                batch_features = model(batch_tensor)\n",
    "                batch_features = batch_features.view(batch_features.size(0), -1)  # Flatten\n",
    "\n",
    "            # Move to CPU & store results\n",
    "            features_list.append(batch_features.cpu().numpy())\n",
    "\n",
    "        # Combine all batch results into a single array\n",
    "        return np.vstack(features_list)\n",
    "\n",
    "    print(\"‚è≥ Extracting CNN feature vectors...\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"Using cpu, this could take a bit.\")\n",
    "    source = extract_features(images, batch_size=16)  # Adjust batch size as needed\n",
    "    print(f\"‚úÖ Extracted {source.shape[1]}D feature vectors for {source.shape[0]} images.\")\n",
    "\n",
    "else:\n",
    "    # i.e. elif FEATURE_METHOD == \"pyramid\":\n",
    "    print(\"‚ö†Ô∏è Using image pyramid instead of CNN features.\")\n",
    "    # Use the images' pixels themselves as their own feature representation. \n",
    "    # Uncomment one of the lines below to experiment with different sources:\n",
    "    # source = images     # Use original images\n",
    "    # source = blurred_0  # Use blurred images\n",
    "    source = pyr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Dimensionality Reduction:\n",
    "### Reducing high-dimensional image descriptions into a 2D \"Embedding\" with UMAP or T-SNE\n",
    "\n",
    "High-dimensional image feature vectors are mapped into a 2D space using UMAP or T-SNE, techniques that preserve visual similarity. This step organizes images based on their features, clustering similar ones together. The resulting 2D space, called an \"embedding\" serves as a layout for arranging images in a meaningful way in the final visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# APPLY UMAP OR T-SNE FOR DIMENSIONALITY REDUCTION\n",
    "# Reduces high-dimensional image data into a 2D space.\n",
    "# Note: This cell can take some time ‚Äî e.g. ~5s for 1K points\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1Ô∏è‚É£ Import necessary libraries\n",
    "\n",
    "import json\n",
    "import os\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2Ô∏è‚É£ CONFIGURE UMAP OR T-SNE PARAMETERS\n",
    "# You can also add `random_state=12345` to set the UMAP/T-SNE random seed to 12345.\n",
    "print(f\"üîÑ Running {REDUCTION_METHOD} for dimensionality reduction...\")\n",
    "\n",
    "if REDUCTION_METHOD == \"UMAP\":\n",
    "    reducer = UMAP(\n",
    "        n_neighbors=15,     # Larger values (e.g., 50) preserve more global structure\n",
    "        min_dist=0.1,       # Controls how tightly points are packed\n",
    "        n_components=2,     # Number of output dimensions (keep at 2D)\n",
    "        metric=\"euclidean\"  # Other options: \"cosine\", \"manhattan\", \"correlation\"\n",
    "    )\n",
    "    print(f\"‚ö†Ô∏è Note: UMAP may trigger a FutureWarning; you can safely ignore this.\")\n",
    "    \n",
    "elif REDUCTION_METHOD == \"TSNE\":\n",
    "    reducer = TSNE(\n",
    "        n_components=2,      # Number of output dimensions (keep at 2D)\n",
    "        perplexity=30,       # Affects clustering (try values between 5 and 50)\n",
    "        early_exaggeration=12,\n",
    "        learning_rate=200,\n",
    "        init=\"random\"        # Can also use \"pca\"\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå Invalid REDUCTION_METHOD: {REDUCTION_METHOD}. Choose 'UMAP' or 'TSNE'.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3Ô∏è‚É£ RUN UMAP OR T-SNE & GENERATE EMBEDDING\n",
    "# An \"embedding\" is a lower-dimensional representation of the data, \n",
    "# where similar images are placed closer together in the new space.\n",
    "#\n",
    "# ‚ö†Ô∏è Note: UMAP may trigger a FutureWarning related to `force_all_finite`\n",
    "# being renamed to `ensure_all_finite` in a future version of scikit-learn.\n",
    "# This warning is harmless and does NOT affect the results; you can safely ignore it.\n",
    "\n",
    "%time Y = reducer.fit_transform(source.reshape(images.shape[0], -1).astype(np.float64))\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4Ô∏è‚É£ Normalize Output to the range [0,1] for visualization\n",
    "Y -= Y.min(axis=0)\n",
    "Y /= Y.max(axis=0)\n",
    "\n",
    "print(f\"‚úÖ {REDUCTION_METHOD} completed.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5Ô∏è‚É£ SAVE EMBEDDING POSITIONS AS JSON\n",
    "# Save the (x, y) 2D embedding coordinates for each image.\n",
    "# This will generate a file with contents like:\n",
    "# {\n",
    "#  \"image_001.png\": {\"x\": 0.1234, \"y\": 0.5678},\n",
    "#  \"image_002.png\": {\"x\": 0.2345, \"y\": 0.6789},\n",
    "# }\n",
    "\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "embedding_positions = {\n",
    "    filename: {\"x\": float(pos[0]), \"y\": float(pos[1])} \n",
    "    for filename, pos in zip(filenames, Y)\n",
    "}\n",
    "embedding_json_path = os.path.join(output_dir, f\"{REDUCTION_METHOD}_positions.json\")\n",
    "with open(embedding_json_path, \"w\") as f:\n",
    "    json.dump(embedding_positions, f, indent=2)\n",
    "print(f\"üíæ Saved embedding positions to {embedding_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "canvas = plot_images(images, Y, canvas_shape=(2048, 2048, 3), blend=np.minimum, fill=255)\n",
    "imshow(canvas, retina=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Rectifying the 2D Embedding:\n",
    "### Mapping T-SNE/UMAP Points to a Grid by Solving the Linear Assignment Problem\n",
    "\n",
    "The 2D embedding from UMAP or T-SNE arranges images organically, but the points are not perfectly aligned to a grid. To correct this, we solve the Linear Assignment Problem (LAP) using an optimization algorithm. This step assigns each image to the nearest available grid position while preserving relative structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SOLVE THE ASSIGNMENT PROBLEM: \n",
    "# MAPPING UMAP POINTS TO A GRID\n",
    "\n",
    "from lap import lapjv  # Linear Assignment Problem (LAP) solver\n",
    "from scipy.spatial.distance import cdist  # Computes pairwise distances\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1Ô∏è‚É£ Define the target grid dimensions\n",
    "nx = int(img.shape[1] / TILE_SIZE) # Number of grid columns\n",
    "ny = int(img.shape[0] / TILE_SIZE) # Number of grid rows\n",
    "\n",
    "# 2Ô∏è‚É£ Generate a uniform 2D grid of (x, y) target positions\n",
    "xv, yv = np.meshgrid(np.linspace(0, 1, nx), np.linspace(0, 1, ny))  # Evenly spaced grid\n",
    "grid = np.dstack((xv, yv)).reshape(-1, 2)  # Convert to list of 2D points\n",
    "\n",
    "# 3Ô∏è‚É£ Compute the pairwise squared Euclidean distance between UMAP points and grid points\n",
    "#    - `Y` contains the 2D coordinates of images in the UMAP space.\n",
    "#    - `grid` contains the target positions in a structured layout.\n",
    "#    - `cdist()` calculates all pairwise distances.\n",
    "%time cost = cdist(grid, Y, 'sqeuclidean')  # (num_grid_points, num_images)\n",
    "\n",
    "# 4Ô∏è‚É£ Convert cost matrix to integer values (LAP solver works faster with integers)\n",
    "cost = cost * (100000. / cost.max())  # Scale costs to large integers\n",
    "cost = cost.astype(int)  # Convert to integer type\n",
    "\n",
    "# 5Ô∏è‚É£ Solve the Linear Assignment Problem (LAP) using Jonker-Volgenant Algorithm\n",
    "# See: https://en.wikipedia.org/wiki/Hungarian_algorithm. \n",
    "# Here we compute the optimal assignment, and print the execution time \n",
    "# Note: this implementation sorts black padding squares, technically incorrect. \n",
    "totalDataPoints = nx * ny  # Number of points to assign\n",
    "%time min_cost, row_assigns, col_assigns = lapjv(cost, extend_cost=True)\n",
    "\n",
    "# 6Ô∏è‚É£ Map embedding positions (`Y`) to the nearest grid positions (`grid_jv`)\n",
    "grid_jv = grid[col_assigns[:totalDataPoints]]\n",
    "\n",
    "# 7Ô∏è‚É£ Visualization: Draw arrows from embedding positions to assigned grid positions\n",
    "plt.figure(figsize=(8, 8))\n",
    "for start, end in zip(Y, grid_jv):\n",
    "    plt.arrow(start[0], 1 - start[1],  # Flip Y-axis by subtracting from 1\n",
    "              end[0] - start[0], -(end[1] - start[1]),  # Negate the Y difference\n",
    "              head_length=0.01, head_width=0.01)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Save grid positions to JSON.\n",
    "# Output file (grid_positions.json) looks like:\n",
    "# {\n",
    "#   \"grid_size\": {\n",
    "#     \"nx\": 41,\n",
    "#     \"ny\": 26\n",
    "#   },\n",
    "#   \"positions\": {\n",
    "#     \"kress_00001.png\": {\n",
    "#       \"col\": 26,\n",
    "#       \"row\": 3\n",
    "#     },\n",
    "# etc.\n",
    "      \n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "grid_positions = {\n",
    "    \"grid_size\": {\"nx\": nx, \"ny\": ny},  # Store metadata about the grid size\n",
    "    \"positions\": {\n",
    "        filename: {\n",
    "            \"col\": round(pos[0] * (nx - 1)),  # Scale normalized value to grid\n",
    "            \"row\": round(pos[1] * (ny - 1))   # Scale normalized value to grid\n",
    "        }\n",
    "        for filename, pos in zip(filenames, grid_jv)\n",
    "    }\n",
    "}\n",
    "grid_json_path = os.path.join(output_dir, \"grid_positions.json\")\n",
    "with open(grid_json_path, \"w\") as f:\n",
    "    json.dump(grid_positions, f, indent=2)\n",
    "print(f\"üíæ Saved grid positions to {grid_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# RECONSTRUCT THE ORDERED MOSAIC\n",
    "\n",
    "# 1Ô∏è‚É£ Swap x and y coordinates to match the (row, column) ordering of the grid\n",
    "grid_tuples = [(y, x) for (x, y) in map(tuple, grid_jv)]\n",
    "\n",
    "# 2Ô∏è‚É£ Pair each image with its corresponding grid position\n",
    "image_grid_pairs = list(zip(grid_tuples, images))\n",
    "\n",
    "# 3Ô∏è‚É£ Sort the image-grid pairs by grid position (top-left to bottom-right)\n",
    "sorted_pairs = sorted(image_grid_pairs)  # Sorts by (y, x) automatically\n",
    "\n",
    "# 4Ô∏è‚É£ Extract the images from the sorted pairs (now correctly ordered for the mosaic).\n",
    "# Note that the position data is now ignored.\n",
    "sorted_images = [image for (position, image) in sorted_pairs]\n",
    "\n",
    "# 5Ô∏è‚É£ Arrange sorted images into a final mosaic nparray\n",
    "mosaic = make_mosaic(sorted_images, nx=nx, ny=ny)\n",
    "\n",
    "# 6Ô∏è‚É£ Display the final, neatly arranged mosaic (TRUE SIZE)\n",
    "# Note: `imshow(mosaic, retina=True)` is not pixel-perfect.\n",
    "import PIL.Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Save the mosaic as an image file\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_filename = \"final_mosaic.png\"\n",
    "# Save the mosaic in the output directory\n",
    "output_filename = os.path.join(output_dir, output_filename)\n",
    "PIL.Image.fromarray(mosaic).save(output_filename)\n",
    "\n",
    "# 7Ô∏è‚É£ (Force Jupyter to) display the image at its true size using HTML\n",
    "# Otherwise Jupyter may resize the image and add a dumb border. \n",
    "display(HTML(f'<img src=\"{output_filename}\" width=\"{mosaic.shape[1]}\" height=\"{mosaic.shape[0]}\" style=\"border:0px;\">'))\n",
    "print(f\"‚úÖ Generated UMAP mosaic with dimensions: {mosaic.shape[1]} x {mosaic.shape[0]} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
